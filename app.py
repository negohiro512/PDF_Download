import streamlit as st
import os
import time
import urllib.parse
import requests
import shutil
import tempfile
from bs4 import BeautifulSoup

# --- ç”»é¢ã®ã‚¿ã‚¤ãƒˆãƒ« ---
st.title("ç¦å²¡å¸‚ç”£å»ƒPDF ä¸€æ‹¬ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ€ãƒ¼")
st.write("æŒ‡å®šã—ãŸURLã‹ã‚‰ã€æ¡ä»¶ã«åˆã†PDFã‚’ã¾ã¨ã‚ã¦ZIPã§ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚")

# --- ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›æ¬„ ---
target_url = st.text_input("å¯¾è±¡ã®URL", "https://www.city.fukuoka.lg.jp/kankyo/sanhai/hp/sangyouhaikibutu/haisyutujigyousya/taryoukouhyoua.html")
keyword = st.text_input("ãƒ•ã‚¡ã‚¤ãƒ«åã«å«ã‚€æ–‡å­—ï¼ˆç©ºæ¬„ãªã‚‰ã™ã¹ã¦ï¼‰", "06")

# --- å®Ÿè¡Œãƒœã‚¿ãƒ³ ---
if st.button("ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã‚’é–‹å§‹"):
    # é€²æ—ãƒãƒ¼ã®è¡¨ç¤º
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    # ä¸€æ™‚ãƒ•ã‚©ãƒ«ãƒ€ã‚’ä½œæˆï¼ˆå‡¦ç†ãŒçµ‚ã‚ã£ãŸã‚‰è‡ªå‹•ã§æ¶ˆãˆã‚‹ã‚ˆã†ã«ã™ã‚‹ï¼‰
    with tempfile.TemporaryDirectory() as temp_dir:
        save_dir = os.path.join(temp_dir, "pdfs")
        os.makedirs(save_dir)
        
        status_text.text("ã‚µã‚¤ãƒˆã®æƒ…å ±ã‚’å–å¾—ä¸­...")
        
        try:
            response = requests.get(target_url)
            response.encoding = response.apparent_encoding
            soup = BeautifulSoup(response.text, "html.parser")
            
            links = soup.find_all("a")
            pdf_links = [l for l in links if l.get("href") and l.get("href").lower().endswith(".pdf")]
            
            total_links = len(pdf_links)
            count = 0
            
            for i, link in enumerate(pdf_links):
                href = link.get("href")
                pdf_url = urllib.parse.urljoin(target_url, href)
                filename = os.path.basename(pdf_url)
                
                # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ¤å®š
                if keyword in filename:
                    status_text.text(f"ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {filename}")
                    
                    try:
                        pdf_data = requests.get(pdf_url)
                        save_path = os.path.join(save_dir, filename)
                        with open(save_path, "wb") as f:
                            f.write(pdf_data.content)
                        count += 1
                        time.sleep(1) # ãƒãƒŠãƒ¼ã¨ã—ã¦å¾…æ©Ÿ
                    except Exception as e:
                        st.error(f"ã‚¨ãƒ©ãƒ¼: {filename} - {e}")
                
                # é€²æ—ãƒãƒ¼ã®æ›´æ–°
                progress_bar.progress((i + 1) / total_links)

            if count > 0:
                status_text.text("ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆä¸­...")
                # ZIPãƒ•ã‚¡ã‚¤ãƒ«ã®ä½œæˆ
                zip_path = os.path.join(temp_dir, "download_files")
                shutil.make_archive(zip_path, 'zip', root_dir=save_dir)
                
                # ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ãƒœã‚¿ãƒ³ã®è¡¨ç¤º
                with open(zip_path + ".zip", "rb") as f:
                    st.download_button(
                        label="ğŸ“¦ ZIPãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰",
                        data=f,
                        file_name="fukuoka_pdfs.zip",
                        mime="application/zip"
                    )
                st.success(f"å®Œäº†ã—ã¾ã—ãŸï¼ {count} å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã¾ã¨ã‚ã¾ã—ãŸã€‚")
            else:
                st.warning(f"ã€Œ{keyword}ã€ã‚’å«ã‚€PDFã¯è¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚")
                
        except Exception as e:
            st.error(f"ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
